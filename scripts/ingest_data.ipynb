{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bz2\n",
    "import shutil\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound, Forbidden\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38767fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"  # Spoof a browser or wget\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de03579",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = \"./tmp/\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you authenticated through the GCP SDK you can comment out these two lines\n",
    "CREDENTIALS_FILE = os.path.join(os.getcwd(), \"credentials.json\")\n",
    "client = storage.Client.from_service_account_json(CREDENTIALS_FILE)\n",
    "# If commented initialize client with the following\n",
    "# client = storage.Client(project='data-expo-pipeline')\n",
    "\n",
    "BUCKET_NAME = \"data_expo_bucket\"\n",
    "CHUNK_SIZE = 16 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEURL = \"https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/\"\n",
    "data_url = {\"YGU3TD\": 2000, \"CI5CEM\": 2001, \"OWJXH3\": 2002, \"KM2QOA\": 2003, \"CCAZGT\": 2004, \"JTFT25\": 2005, \"EPIFFT\": 2006, \"2BHLWK\": 2007, \"EIR0RA\": 2008}\n",
    "information_url = {\"XTPZZY\": \"airports\", \"3NOQ6Q\": \"carriers\", \"XXSL8A\": \"plane\", \"YZWKHN\": \"variable_descriptions\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yearly_file(year_hash: str):\n",
    "    \"\"\"\n",
    "    Download BZ2 files, then extract them to get CSV files.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    year_hash: str\n",
    "        A string key corresponding to the date value in data_url dict.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    csv_path: str\n",
    "        If path to CSV file is available, return it.\n",
    "    None\n",
    "        If there is an exception.\n",
    "    \"\"\"\n",
    "\n",
    "    url = BASEURL + year_hash\n",
    "    bz2_path = os.path.join(DOWNLOAD_DIR, f\"{data_url[year_hash]}_data.csv.bz2\")\n",
    "    csv_path = os.path.join(DOWNLOAD_DIR, f\"{data_url[year_hash]}_data.csv\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {url}...\")\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(bz2_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(f\"Downloaded: {bz2_path}\")\n",
    "\n",
    "        print(f\"Unzipping {bz2_path}...\")\n",
    "        with bz2.open(bz2_path, 'rb') as file_in:\n",
    "            with open(csv_path, 'wb') as file_out:\n",
    "                shutil.copyfileobj(file_in, file_out)\n",
    "\n",
    "        print(\"Unzip completed\")\n",
    "\n",
    "        print(\"Removing redundant gz files...\")\n",
    "        os.remove(bz2_path)\n",
    "        print(\"gz files removed!\")\n",
    "\n",
    "        return csv_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_information_file(hash: str):\n",
    "    \"\"\"\n",
    "    Download CSV helper files that contain detailed information.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    hash: str\n",
    "        A string key corresponding to the name value in information_url dict.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    csv_path: str\n",
    "        If path to CSV file is available, return it.\n",
    "    None\n",
    "        If there is an exception.\n",
    "    \"\"\"\n",
    "\n",
    "    url = BASEURL + hash\n",
    "    csv_path = os.path.join(DOWNLOAD_DIR, f\"{information_url[hash]}_data.csv\")\n",
    "    try:\n",
    "        print(f\"Downloading {url}...\")\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(csv_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(f\"Downloaded: {csv_path}\")\n",
    "\n",
    "        return csv_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name: str):\n",
    "    \"\"\"\n",
    "    Create a bucket if not exists. \n",
    "    Only proceeds if the bucket exists and belong to the current project.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket_name: str\n",
    "        Name of the bucket to construct a bucket object.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bucket: google.cloud.storage.bucket.Bucket\n",
    "        A bucket matching the name provided.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bucket = client.get_bucket(bucket_name)      \n",
    "        bucket_ids = [bckt.id for bckt in client.list_buckets()]        \n",
    "        \n",
    "        # Verify that the bucket exists in the current project\n",
    "        if bucket_name in bucket_ids:\n",
    "            print(f\"Bucket '{bucket_name}' exists and belongs to your project. Proceeding...\")\n",
    "            return bucket\n",
    "        else:\n",
    "            print(f\"A bucket with the name '{bucket_name}' already exists, but it does not belong to your project.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    except NotFound:\n",
    "        # If the bucket doesn't exist, create it\n",
    "        bucket = client.create_bucket(bucket_name)\n",
    "        print(f\"Created bucket '{bucket_name}'\")\n",
    "        return bucket\n",
    "\n",
    "    except Forbidden:\n",
    "        # If the request is forbidden, it means the bucket exists but you don't have access to see details\n",
    "        print(f\"A bucket with the name '{bucket_name}' exists, but it is not accessible. Bucket name is taken. Please try a different bucket name.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(file_path, bucket, max_retries=3):\n",
    "    \"\"\"\n",
    "    Upload files from local to Google Cloud Storage (GCS).\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    file_path: str\n",
    "        Path to the CSV file to upload.    \n",
    "    bucket: google.cloud.storage.bucket.Bucket\n",
    "        A bucket matching the name provided.\n",
    "    max_retries: int\n",
    "        Number of retries in case uploading fails.\n",
    "    \"\"\"\n",
    "\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.chunk_size = CHUNK_SIZE \n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Uploading {file_path} to {BUCKET_NAME} (Attempt {attempt + 1})...\")\n",
    "            blob.upload_from_filename(file_path)\n",
    "            print(f\"Uploaded: gs://{BUCKET_NAME}/{blob_name}\")\n",
    "\n",
    "            if storage.Blob(name=blob_name, bucket=bucket).exists(client):\n",
    "                print(f\"Verification successful for {blob_name}\")\n",
    "                return\n",
    "            else:\n",
    "                print(f\"Verification failed for {blob_name}, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {file_path} to GCS: {e}\")\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(f\"Giving up on {file_path} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1481a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files(dir_path):\n",
    "    \"\"\"\n",
    "    Remove the directory containing all the CSV files after uploading to GCS.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    dir_path: str\n",
    "        Path to the directory.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"Removed directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    yearly_files = list(executor.map(download_yearly_file, data_url.keys()))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    info_files = list(executor.map(download_information_file, information_url.keys()))\n",
    "\n",
    "all_files = list(filter(None, yearly_files + info_files))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(upload_to_gcs, path, create_bucket(BUCKET_NAME)) for path in all_files]\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_files(\"./tmp/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
