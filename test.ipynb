{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e61261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bz2\n",
    "import shutil\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound, Forbidden\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38767fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"  # Spoof a browser or wget\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e9ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you authenticated through the GCP SDK you can comment out these two lines\n",
    "CREDENTIALS_FILE = os.path.join(os.getcwd(), \"credentials.json\")\n",
    "client = storage.Client.from_service_account_json(CREDENTIALS_FILE)\n",
    "# If commented initialize client with the following\n",
    "# client = storage.Client(project='data-expo-pipeline')\n",
    "\n",
    "BUCKET_NAME = \"data_expo_bucket\"\n",
    "CHUNK_SIZE = 16 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da6b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEURL = \"https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/\"\n",
    "data_url = {\"YGU3TD\": 2000, \"CI5CEM\": 2001, \"OWJXH3\": 2002, \"KM2QOA\": 2003, \"CCAZGT\": 2004, \"JTFT25\": 2005, \"EPIFFT\": 2006, \"2BHLWK\": 2007, \"EIR0RA\": 2008}\n",
    "information_url = {\"XTPZZY\": \"airports\", \"3NOQ6Q\": \"carriers\", \"XXSL8A\": \"plane\", \"YZWKHN\": \"variable_descriptions\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a526f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = \".\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e46564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yearly_file(year_hash: str):\n",
    "    url = BASEURL + year_hash\n",
    "    bz2_path = os.path.join(DOWNLOAD_DIR, f\"{data_url[year_hash]}_data.csv.bz2\")\n",
    "    csv_path = os.path.join(DOWNLOAD_DIR, f\"{data_url[year_hash]}_data.csv\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {url}...\")\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(bz2_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(f\"Downloaded: {bz2_path}\")\n",
    "        \n",
    "        print(f\"Unzipping {bz2_path}...\")\n",
    "        with bz2.open(bz2_path, 'rb') as file_in:\n",
    "            with open(csv_path, 'wb') as file_out:\n",
    "                shutil.copyfileobj(file_in, file_out)\n",
    "\n",
    "        print(\"Unzip completed\")\n",
    "\n",
    "        print(\"Removing redundant gz files...\")\n",
    "        os.remove(bz2_path)\n",
    "        print(\"gz files removed!\")\n",
    "\n",
    "        return csv_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0cd48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_information_file(hash: str):\n",
    "    url = BASEURL + hash\n",
    "    csv_path = os.path.join(DOWNLOAD_DIR, f\"{information_url[hash]}_data.csv\")\n",
    "    try:\n",
    "        print(f\"Downloading {url}...\")\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(csv_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(f\"Downloaded: {csv_path}\")\n",
    "\n",
    "        return csv_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464a02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name: str):\n",
    "    \"\"\"\n",
    "    Create a bucket if not exists. \n",
    "    Only proceeds if the bucket exists and belong to the current project.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket_name: str\n",
    "        Name of the bucket to construct a bucket object.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bucket: google.cloud.storage.bucket.Bucket\n",
    "        A bucket matching the name provided.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bucket = client.get_bucket(bucket_name)      \n",
    "        bucket_ids = [bckt.id for bckt in client.list_buckets()]        \n",
    "        \n",
    "        # Verify that the bucket exists in the current project\n",
    "        if bucket_name in bucket_ids:\n",
    "            print(f\"Bucket '{bucket_name}' exists and belongs to your project. Proceeding...\")\n",
    "            return bucket\n",
    "        else:\n",
    "            print(f\"A bucket with the name '{bucket_name}' already exists, but it does not belong to your project.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    except NotFound:\n",
    "        # If the bucket doesn't exist, create it\n",
    "        bucket = client.create_bucket(bucket_name)\n",
    "        print(f\"Created bucket '{bucket_name}'\")\n",
    "        return bucket\n",
    "\n",
    "    except Forbidden:\n",
    "        # If the request is forbidden, it means the bucket exists but you don't have access to see details\n",
    "        print(f\"A bucket with the name '{bucket_name}' exists, but it is not accessible. Bucket name is taken. Please try a different bucket name.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e224cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(file_path, bucket, max_retries=3):\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.chunk_size = CHUNK_SIZE \n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Uploading {file_path} to {BUCKET_NAME} (Attempt {attempt + 1})...\")\n",
    "            blob.upload_from_filename(file_path)\n",
    "            print(f\"Uploaded: gs://{BUCKET_NAME}/{blob_name}\")\n",
    "\n",
    "            if storage.Blob(name=blob_name, bucket=bucket).exists(client):\n",
    "                print(f\"Verification successful for {blob_name}\")\n",
    "                return\n",
    "            else:\n",
    "                print(f\"Verification failed for {blob_name}, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {file_path} to GCS: {e}\")\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(f\"Giving up on {file_path} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320b3708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/YGU3TD...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/CI5CEM...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/OWJXH3...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/KM2QOA...\n",
      "Downloaded: ./2002_data.csv.bz2\n",
      "Unzipping ./2002_data.csv.bz2...\n",
      "Downloaded: ./2000_data.csv.bz2\n",
      "Unzipping ./2000_data.csv.bz2...\n",
      "Downloaded: ./2001_data.csv.bz2\n",
      "Unzipping ./2001_data.csv.bz2...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/CCAZGT...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/JTFT25...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/EPIFFT...\n",
      "Downloaded: ./2003_data.csv.bz2\n",
      "Unzipping ./2003_data.csv.bz2...\n",
      "Downloaded: ./2004_data.csv.bz2\n",
      "Unzipping ./2004_data.csv.bz2...\n",
      "Downloaded: ./2005_data.csv.bz2\n",
      "Unzipping ./2005_data.csv.bz2...\n",
      "Downloaded: ./2006_data.csv.bz2\n",
      "Unzipping ./2006_data.csv.bz2...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/2BHLWK...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/EIR0RA...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloaded: ./2008_data.csv.bz2\n",
      "Unzipping ./2008_data.csv.bz2...\n",
      "Downloaded: ./2007_data.csv.bz2\n",
      "Unzipping ./2007_data.csv.bz2...\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Unzip completed\n",
      "Removing redundant gz files...\n",
      "gz files removed!\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/XTPZZY...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/3NOQ6Q...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/XXSL8A...\n",
      "Downloading https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/YZWKHN...\n",
      "Downloaded: ./variable_descriptions_data.csv\n",
      "Downloaded: ./carriers_data.csv\n",
      "Downloaded: ./airports_data.csv\n",
      "Downloaded: ./plane_data.csv\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Uploading ./2000_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Uploading ./2001_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Uploading ./2002_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Uploading ./2003_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Bucket 'data_expo_bucket' exists and belongs to your project. Proceeding...\n",
      "Uploaded: gs://data_expo_bucket/2003_data.csv\n",
      "Verification successful for 2003_data.csv\n",
      "Uploading ./2004_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/2002_data.csv\n",
      "Verification successful for 2002_data.csv\n",
      "Uploading ./2005_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/2001_data.csv\n",
      "Verification successful for 2001_data.csv\n",
      "Uploading ./2006_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/2000_data.csv\n",
      "Verification successful for 2000_data.csv\n",
      "Uploading ./2007_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/2004_data.csv\n",
      "Verification successful for 2004_data.csv\n",
      "Uploading ./2008_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/2005_data.csv\n",
      "Verification successful for 2005_data.csv\n",
      "Uploading ./airports_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/airports_data.csv\n",
      "Verification successful for airports_data.csv\n",
      "Uploading ./carriers_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/carriers_data.csv\n",
      "Verification successful for carriers_data.csv\n",
      "Uploading ./plane_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/plane_data.csv\n",
      "Verification successful for plane_data.csv\n",
      "Uploading ./variable_descriptions_data.csv to data_expo_bucket (Attempt 1)...\n",
      "Uploaded: gs://data_expo_bucket/variable_descriptions_data.csv\n",
      "Verification successful for variable_descriptions_data.csv\n",
      "Uploaded: gs://data_expo_bucket/2008_data.csv\n",
      "Verification successful for 2008_data.csv\n",
      "Uploaded: gs://data_expo_bucket/2006_data.csv\n",
      "Verification successful for 2006_data.csv\n",
      "Uploaded: gs://data_expo_bucket/2007_data.csv\n",
      "Verification successful for 2007_data.csv\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    yearly_files = list(executor.map(download_yearly_file, data_url.keys()))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    info_files = list(executor.map(download_information_file, information_url.keys()))\n",
    "\n",
    "all_files = list(filter(None, yearly_files + info_files))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(upload_to_gcs, path, create_bucket(BUCKET_NAME)) for path in all_files]\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux_deproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
